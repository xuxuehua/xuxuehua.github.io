<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/static/css/tango.css">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>map_reduce - Xu XueHua</title>
    <meta name="keywords" content="Xu XueHua"/>
    <meta name="description" content="Xu XueHua's public notes"/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
    <div id="container">
      
<div id="header">
  <div class="post-nav"><a href="/">Home</a>&nbsp;&#187;&nbsp;<a href="/#Big_data">Big_data</a>&nbsp;&#187;&nbsp;map_reduce
    <span class="updated">Page Updated&nbsp;
      2019-06-12 15:54
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">map_reduce</div>

  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">分布式计算</a></li>
<li><a href="#map-reduce">Map Reduce 模型</a><ul>
<li><a href="#_2">处理过程</a></li>
</ul>
</li>
<li><a href="#_3">作业启动</a><ul>
<li><a href="#_4">大数据应用进程</a></li>
<li><a href="#_5"></a></li>
<li><a href="#jobtracker">JobTracker进程</a></li>
<li><a href="#tasktracker">TaskTracker进程</a></li>
</ul>
</li>
<li><a href="#_6">运行机制</a><ul>
<li><a href="#shuffle">shuffle</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="_1">分布式计算</h1>
<p>Hadoop MapReduce的出现，使得大数据计算通用编程成为可能。我们只要遵循MapReduce编程模型编写业务处理逻辑代码，就可以运行在Hadoop分布式集群上，无需关心分布式计算是如何完成的。也就是说，我们只需要关心业务逻辑，不用关心系统调用与运行环境，这和我们目前的主流开发方式是一致的。</p>
<h1 id="map-reduce">Map Reduce 模型</h1>
<p><strong>MapReduce既是一个编程模型，又是一个计算框架</strong>。也就是说，开发人员必须基于MapReduce编程模型进行编程开发，然后将程序通过MapReduce计算框架分发到Hadoop集群中运行</p>
<p>该编程模型只包含Map和Reduce两个过程，map的主要输入是一对<Key, Value>值，经过map计算后输出一对<Key, Value>值；然后将相同Key合并，形成<Key, Value集合>；再将这个<Key, Value集合>输入reduce，经过计算输出零个或多个<Key, Value>对。</p>
<p>同时，MapReduce又是非常强大的，不管是关系代数运算（SQL计算），还是矩阵运算（图计算），大数据领域几乎所有的计算需求都可以通过MapReduce编程来实现</p>
<h2 id="_2">处理过程</h2>
<p>WordCount主要解决的是文本处理中词频统计的问题，就是统计文本中每一个单词出现的次数。如果只是统计一篇文章的词频，几十KB到几MB的数据，只需要写一个程序，将数据读入内存，建一个Hash表记录每个词出现的次数就可以了。</p>
<p><img alt="img" src="https://snag.gy/hw714m.jpg" /></p>
<div class="hlcode"><pre><span class="c"># 文本前期处理</span>
<span class="n">strl_ist</span> <span class="o">=</span> <span class="nb">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\n</span><span class="s">&#39;</span><span class="p">,</span> <span class="s">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39; &#39;</span><span class="p">)</span>
<span class="n">count_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="c"># 如果字典里有该单词则加1，否则添加入字典</span>
<span class="k">for</span> <span class="nb">str</span> <span class="ow">in</span> <span class="n">strl_ist</span><span class="p">:</span>
<span class="k">if</span> <span class="nb">str</span> <span class="ow">in</span> <span class="n">count_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="n">count_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">count_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">count_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>


<p>简单说来，就是建一个Hash表，然后将字符串里的每个词放到这个Hash表里。如果这个词第一次放到Hash表，就新建一个Key、Value对，Key是这个词，Value是1。如果Hash表里已经有这个词了，那么就给这个词的Value + 1。</p>
<p>小数据量用单机统计词频很简单，但是如果想统计全世界互联网所有网页（数万亿计）的词频数（而这正是Google这样的搜索引擎的典型需求），不可能写一个程序把全世界的网页都读入内存，这时候就需要用MapReduce编程来解决</p>
<p>WordCount的MapReduce程序如下</p>
<div class="hlcode"><pre><span class="k">public</span> <span class="nf">class</span> <span class="nx">WordCount</span> <span class="p">{</span>

  <span class="k">public</span> <span class="nf">static</span> <span class="nb">class</span> <span class="nx">TokenizerMapper</span>
       <span class="nx">extends</span> <span class="nx">Mapper</span><span class="o">&lt;</span><span class="nb">Object</span><span class="p">,</span> <span class="nx">Text</span><span class="p">,</span> <span class="nx">Text</span><span class="p">,</span> <span class="nx">IntWritable</span><span class="o">&gt;</span><span class="p">{</span>

    <span class="k">private</span> <span class="nf">final</span> <span class="nx">static</span> <span class="nx">IntWritable</span> <span class="n">one</span> <span class="o">=</span> <span class="nb">new</span> <span class="nx">IntWritable</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="k">private</span> <span class="nf">Text</span> <span class="n">word</span> <span class="o">=</span> <span class="nb">new</span> <span class="nx">Text</span><span class="p">();</span>

    <span class="k">public</span> <span class="nf">void</span> <span class="kt">map</span><span class="p">(</span><span class="nb">Object</span> <span class="nb">key</span><span class="p">,</span> <span class="nx">Text</span> <span class="nb">value</span><span class="p">,</span> <span class="nx">Context</span> <span class="nx">context</span>
                    <span class="p">)</span> <span class="nx">throws</span> <span class="nx">IOException</span><span class="p">,</span> <span class="nx">InterruptedException</span> <span class="p">{</span>
      <span class="nx">StringTokenizer</span> <span class="n">itr</span> <span class="o">=</span> <span class="nb">new</span> <span class="nx">StringTokenizer</span><span class="p">(</span><span class="nx">value.toString</span><span class="p">());</span>
      <span class="k">while</span> <span class="p">(</span><span class="nx">itr.hasMoreTokens</span><span class="p">())</span> <span class="p">{</span>
        <span class="nx">word.set</span><span class="p">(</span><span class="nx">itr.nextToken</span><span class="p">());</span>
        <span class="nx">context.write</span><span class="p">(</span><span class="nx">word</span><span class="p">,</span> <span class="nx">one</span><span class="p">);</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">public</span> <span class="nf">static</span> <span class="nb">class</span> <span class="nx">IntSumReducer</span>
       <span class="nx">extends</span> <span class="nx">Reducer</span><span class="o">&lt;</span><span class="nx">Text</span><span class="p">,</span><span class="nx">IntWritable</span><span class="p">,</span><span class="nx">Text</span><span class="p">,</span><span class="nx">IntWritable</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">private</span> <span class="nf">IntWritable</span> <span class="n">result</span> <span class="o">=</span> <span class="nb">new</span> <span class="nx">IntWritable</span><span class="p">();</span>

    <span class="k">public</span> <span class="nf">void</span> <span class="nx">reduce</span><span class="p">(</span><span class="nx">Text</span> <span class="nb">key</span><span class="p">,</span> <span class="nx">Iterable</span><span class="o">&lt;</span><span class="nx">IntWritable</span><span class="o">&gt;</span> <span class="nb">values</span><span class="p">,</span>
                       <span class="nx">Context</span> <span class="nx">context</span>
                       <span class="p">)</span> <span class="nx">throws</span> <span class="nx">IOException</span><span class="p">,</span> <span class="nx">InterruptedException</span> <span class="p">{</span>
      <span class="nx">int</span> <span class="k">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
      <span class="nb">for</span> <span class="p">(</span><span class="nx">IntWritable</span> <span class="nx">val</span> <span class="p">:</span> <span class="nb">values</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">sum</span> <span class="o">+=</span> <span class="nx">val.get</span><span class="p">();</span>
      <span class="p">}</span>
      <span class="nx">result.set</span><span class="p">(</span><span class="k">sum</span><span class="p">);</span>
      <span class="nx">context.write</span><span class="p">(</span><span class="nb">key</span><span class="p">,</span> <span class="nb">result</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>map函数的输入主要是一个<Key, Value>对，在这个例子里，Value是要统计的所有文本中的一行数据，Key在一般计算中都不会用到。</p>
<div class="hlcode"><pre><span class="n">public</span> <span class="kt">void</span> <span class="n">map</span><span class="p">(</span><span class="n">Object</span> <span class="n">key</span><span class="p">,</span> <span class="n">Text</span> <span class="n">value</span><span class="p">,</span> <span class="n">Context</span> <span class="n">context</span>
                    <span class="p">)</span>
</pre></div>


<p>map函数的计算过程是，将这行文本中的单词提取出来，针对每个单词输出一个<word, 1>这样的<Key, Value>对。</p>
<p>MapReduce计算框架会将这些<word , 1>收集起来，将相同的word放在一起，形成<word , <1,1,1,1,1,1,1…>&gt;这样的<Key, Value集合>数据，然后将其输入给reduce函数。</p>
<div class="hlcode"><pre><span class="k">public</span> <span class="nf">void</span> <span class="nx">reduce</span><span class="p">(</span><span class="nx">Text</span> <span class="nb">key</span><span class="p">,</span> <span class="nx">Iterable</span><span class="o">&lt;</span><span class="nx">IntWritable</span><span class="o">&gt;</span> <span class="nb">values</span><span class="p">,</span>
                       <span class="nx">Context</span> <span class="nx">context</span>
                       <span class="p">)</span> 
</pre></div>


<p>这里reduce的输入参数Values就是由很多个1组成的集合，而Key就是具体的单词word。</p>
<p>reduce函数的计算过程是，将这个集合里的1求和，再将单词（word）和这个和（sum）组成一个<Key, Value>，也就是<word, sum>输出。每一个输出就是一个单词和它的词频统计总和。</p>
<p>一个map函数可以针对一部分数据进行运算，这样就可以将一个大数据切分成很多块（这也正是HDFS所做的），MapReduce计算框架为每个数据块分配一个map函数去计算，从而实现大数据的分布式计算。</p>
<h1 id="_3">作业启动</h1>
<p><img alt="img" src="https://snag.gy/EwucmX.jpg" /></p>
<p>以Hadoop 1为例，MapReduce运行过程涉及三类关键进程</p>
<h2 id="_4">大数据应用进程</h2>
<p>这类进程是启动MapReduce程序的主入口，主要是指定Map和Reduce类、输入输出文件路径等，并提交作业给Hadoop集群，也就是下面提到的JobTracker进程。这是由用户启动的MapReduce程序进程，比如我们上期提到的WordCount程序。</p>
<h2 id="_5"></h2>
<h2 id="jobtracker">JobTracker进程</h2>
<p>这类进程根据要处理的输入数据量，命令下面提到的TaskTracker进程启动相应数量的Map和Reduce进程任务，并管理整个作业生命周期的任务调度和监控。这是Hadoop集群的常驻进程，需要注意的是，JobTracker进程在整个Hadoop集群全局唯一。</p>
<h2 id="tasktracker">TaskTracker进程</h2>
<p>这个进程负责启动和管理Map进程以及Reduce进程。因为需要每个数据块都有对应的map函数，TaskTracker进程通常和HDFS的DataNode进程启动在同一个服务器。也就是说，Hadoop集群中绝大多数服务器同时运行DataNode进程和TaskTracker进程。</p>
<p>JobTracker进程和TaskTracker进程是主从关系，主服务器通常只有一台（或者另有一台备机提供高可用服务，但运行时只有一台服务器对外提供服务，真正起作用的只有一台），从服务器可能有几百上千台，所有的从服务器听从主服务器的控制和调度安排。主服务器负责为应用程序分配服务器资源以及作业执行的调度，而具体的计算操作则在从服务器上完成。</p>
<h1 id="_6">运行机制</h1>
<p><img alt="img" src="https://snag.gy/d1NEpn.jpg" /></p>
<p>1.应用进程JobClient将用户作业JAR包存储在HDFS中，将来这些JAR包会分发给Hadoop集群中的服务器执行MapReduce计算。</p>
<p>2.应用程序提交job作业给JobTracker。</p>
<p>3.JobTracker根据作业调度策略创建JobInProcess树，每个作业都会有一个自己的JobInProcess树。</p>
<p>4.JobInProcess根据输入数据分片数目（通常情况就是数据块的数目）和设置的Reduce数目创建相应数量的TaskInProcess。</p>
<p>5.TaskTracker进程和JobTracker进程进行定时通信。</p>
<p>6.如果TaskTracker有空闲的计算资源（有空闲CPU核心），JobTracker就会给它分配任务。分配任务的时候会根据TaskTracker的服务器名字匹配在同一台机器上的数据块计算任务给它，使启动的计算任务正好处理本机上的数据，以实现我们一开始就提到的“移动计算比移动数据更划算”。</p>
<p>7.TaskTracker收到任务后根据任务类型（是Map还是Reduce）和任务参数（作业JAR包路径、输入数据文件路径、要处理的数据在文件中的起始位置和偏移量、数据块多个备份的DataNode主机名等），启动相应的Map或者Reduce进程。</p>
<p>8.Map或者Reduce进程启动后，检查本地是否有要执行任务的JAR包文件，如果没有，就去HDFS上下载，然后加载Map或者Reduce代码开始执行。</p>
<p>9.如果是Map进程，从HDFS读取数据（通常要读取的数据块正好存储在本机）；如果是Reduce进程，将结果数据写出到HDFS。</p>
<p>做的仅仅是编写一个map函数和一个reduce函数就可以了，根本不用关心这两个函数是如何被分布启动到集群上的，也不用关心数据块又是如何分配给计算任务的。<strong>这一切都由MapReduce计算框架完成</strong></p>
<h2 id="shuffle">shuffle</h2>
<p>在map输出与reduce输入之间，MapReduce计算框架处理数据合并与连接操作，这个操作有个专门的词汇叫<strong>shuffle</strong></p>
<p><strong>分布式计算需要将不同服务器上的相关数据合并到一起进行下一步计算，这就是shuffle</strong>。</p>
<p><img alt="img" src="https://snag.gy/NJDsAq.jpg" /></p>
<p>每个Map任务的计算结果都会写入到本地文件系统，等Map任务快要计算完成的时候，MapReduce计算框架会启动shuffle过程，在Map任务进程调用一个Partitioner接口，对Map产生的每个<Key, Value>进行Reduce分区选择，然后通过HTTP通信发送给对应的Reduce进程。这样不管Map位于哪个服务器节点，相同的Key一定会被发送给相同的Reduce进程。Reduce任务进程对收到的<Key, Value>进行排序和合并，相同的Key放在一起，组成一个<Key, Value集合>传递给Reduce执行。</p>
<p>map输出的<Key, Value>shuffle到哪个Reduce进程是这里的关键，它是由Partitioner来实现，MapReduce框架默认的Partitioner用Key的哈希值对Reduce任务数量取模，相同的Key一定会落在相同的Reduce任务ID上。</p>
<p>从实现上来看的话，这样的Partitioner代码只需要一行。</p>
<div class="hlcode"><pre> <span class="cm">/** Use {@link Object#hashCode()} to partition. */</span> 
<span class="n">public</span> <span class="kt">int</span> <span class="nf">getPartition</span><span class="p">(</span><span class="n">K2</span> <span class="n">key</span><span class="p">,</span> <span class="n">V2</span> <span class="n">value</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numReduceTasks</span><span class="p">)</span> <span class="p">{</span> 
    <span class="k">return</span> <span class="p">(</span><span class="n">key</span><span class="p">.</span><span class="n">hashCode</span><span class="p">()</span> <span class="o">&amp;</span> <span class="n">Integer</span><span class="p">.</span><span class="n">MAX_VALUE</span><span class="p">)</span> <span class="o">%</span> <span class="n">numReduceTasks</span><span class="p">;</span> 
 <span class="p">}</span>
</pre></div>


<p>不管是MapReduce还是Spark，只要是大数据批处理计算，一定都会有shuffle过程，只有<strong>让数据关联起来</strong></p>
<p>shuffle也是整个MapReduce过程中最难、最消耗性能的地方，在MapReduce早期代码中，一半代码都是关于shuffle处理的。</p>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2019 Xu XueHua.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2019-08-18 17:28:02</p>
      </span>
    </div>

    
    
  </body>
</html>